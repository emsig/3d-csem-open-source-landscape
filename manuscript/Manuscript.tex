\documentclass[
    paper,
%     manuscript,
%     twocolumn,
%     twoside,
%     revised,
  ]{geophysics}

% TODO's
\usepackage[color=cyan, textsize=footnotesize, textwidth=2.8cm]{todonotes}

% Additional packages
\usepackage[UKenglish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{siunitx}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{flafter}
\usepackage[pdftex, final, allcolors=blue, colorlinks=true]{hyperref}

% Figures
\renewcommand{\figdir}{./figures}

% Own commands
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\emg}[2]{\texttt{emg#1#2}\xspace}
\newcommand{\empymod}{\texttt{empymod}\xspace}
\newcommand{\simpeg}{\texttt{SimPEG}\xspace}
\newcommand{\discretize}{\texttt{discretize}\xspace}
\newcommand{\custem}{\texttt{custEM}\xspace}
\newcommand{\petgem}{\texttt{PETGEM}\xspace}
\newcommand{\ohmm}{\ensuremath{\Omega\,}\text{m}\xspace}

\newcommand{\rmk}[1]{{\color{red}\bfseries #1}}

% TODO's
% For comments in the margin, use:   \mycom[YOURINITIALS]{comment}
% and for inline comments, use:      \imycom[YOURINITIALS]{comment}
\newcommand{\mycom}[2][]{\todo{\textbf{\uppercase{[#1]}}:~#2}}
\newcommand{\imycom}[2][]{\todo[inline]{\textbf{\uppercase{[#1]}}:~#2}}
\newcommand{\itodo}[1]{\todo[inline]{\sffamily #1}}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\begin{document}

% META INFO
\title{Open-source landscape for 3D CSEM modelling}

% ARTICLE-ID
\ms{}

% ADDRESSES
\address{
  \footnotemark[1]TU Delft, Building 23, Stevinweg 1 / PO-box 5048, 2628 CN Delft (NL);\\
  \footnotemark[2]Leibniz Institut for Applied Geophysics, Stilleweg 2, 30655 Hannover (DE);\\
  \footnotemark[3]Barcelona Supercomputing Center (BSC), Nexus II Building c/Jordi Girona, 29, 08034 Barcelona (ES);\\
  \footnotemark[4]University of California Berkeley, Department of Statistics (USA).\\
  E-mail: \href{mailto:Dieter@Werthmuller.org}{Dieter@Werthmuller.org};\\
  \textbf{Keywords}: CSEM, Open-Source, 3D modelling, FV, FE.
}
% AUTHORS
\author{%
  Dieter Werthmüller\footnotemark[1],          % orcid: 0000-0002-8575-2484
  Raphael Rochlitz\footnotemark[2],            % orcid: 0000-0002-5132-916X
  Octavio Castillo-Reyes\footnotemark[3], and  % orcid: 0000-0003-4271-5015
  Lindsey Heagy\footnotemark[4]                % orcid: 0000-0002-1551-5926
}

% HEADER / FOOTER
\footer{}
\lefthead{Werthmüller et al.}
\righthead{Open-source 3D CSEM modelling}

\maketitle

\begin{abstract}
Large-scale modelling of three-dimensional controlled-source electromagnetic (CSEM) responses used to be feasible only for large companies and research consortia. This has changed over the last few years, and today there exists a selection of different open-source codes. Using four different codes in the Python ecosystem we model responses for three increasingly complex models. We first verify the results with semi-analytical solutions for a simple case. Then we validate the responses of more complex models between the different codes and with results from the industry. These validations show that the open-source codes are able to compute CSEM responses for challenging, large-scale models. Our comparison includes finite-element and finite-volume codes using structured tensor and octree meshes as well as unstructured tetrahedra meshes. Our results show further that accurate responses can be obtained independently of the chosen method and the chosen mesh. The runtime and memory requirements vary greatly based on the choice of iterative or direct solvers. But we have found that much more time was spent on creating the appropriate input files than running the actual computation. The challenging task is to appropriately discretize the model, independently of the chosen mesh and the chosen solver. We provide the three models and the corresponding responses of four codes, which can be used for verification of new and existing codes. The collaboration of four code maintainer trying to achieve the same task brought in the end all four codes a significant step further.
\end{abstract}

\newpage
\itodo{\hfill$\Downarrow$\hfill$\Downarrow$\hfill List of things ToDo, Notes, etc\hfill$\Downarrow$\hfill$\Downarrow$\hfill~}
\begin{itemize}
    \item Notes on orthography: I settled on discretize with a «z». My dictionary says that discreti{\color{red}\bfseries z}e is AE and discreti{\color{red}\bfseries s}e is BE, but that «z» is also used in BE in mathematical contexts. Is that fine or any other opinions?
    \item Regarding submission, {\color{red}\bfseries please let me know}:
      \begin{itemize}
        \item If you think the \textbf{order of authors} should be different. The last thing we want in this collaboration is that someone feels unhappy about that.
        \item If you have \textbf{another manuscript under revision with GJI}. When submitting to GJI one has to indicate when you have other manuscripts currently under review with GJI.
        \item For the submission I will create a \textbf{new repository on swung-research}. The idea is to make a single, new commit with everything (without the git history, for various reasons). Please let me know if you would prefer to commit your files yourself as a PR instead.
        \item \textbf{Wanted and not wanted reviewers:} Let me know if you have any names for these. I do not have any \emph{not wanted}, but I do have a few I will put on the \emph{wanted} list, mainly people I would be interested to have their feedback and that have knowledge about 3D modelling:
          \begin{itemize}
            \item Kerry Key (domain knowledge with \texttt{MARE3DEM}, also experience with open-sourcing with \texttt{MARE2DEM}); \imycom[RR]{Agreed}
            \item Rune Mittet (domain knowledge; he will know what SBLwiz actually does, so it would be very interesting to have his feedback);
            \item Dimitry Avdeev and Ralph-Uwe Börner (they both did extensive 3D code overview/review papers in 2005 and 2010, respectively, and it would be interesting to get their feedback). \imycom[RR]{Strongly Agreed}
          \end{itemize}
      \end{itemize}
    \item  {\color{red}\bfseries Everyone: Please ensure that the tables have the correct info:}
      \begin{enumerate}
          \item machine info;
          \item runtime;
          \item memory;
          \item nproc;
          \item dof.
      \end{enumerate}
    I got all the info out of the ".nc" files, so make sure you wrote the correct info into the nc-files. Further notes:
      \begin{description}
        \item[runtime] Should only include the wall time (or elapsed real time, \href{https://en.wikipedia.org/wiki/Elapsed_real_time}{wikipedia.org/\-wiki/\-Elapsed\-\_real\-\_time}) of solving the SLE, for ALL frequencies (in \emg3d some frequencies are slower than others, but in any case, we report the sum of all of them); any pre-/postprocessing such as mesh generation or model interpolation can be ignored.
        \item[memory] It should be the maximum memory increase when solving the SLE (again, ignoring any pre-/postprocessing). Again, of all frequencies. In \emg3d some frequencies require more RAM than others (more iterations), but we only report the "worst", the heaviest consumer.
        \item[nproc] Not sure we reached agreement on Slack, but I took no everything in here (threads and (virtual) processes); please note here if you think it should be different.
        \item[dof] Similarly, not sure we reached agreement on Slack. I put it here now for complex value, hence halving the dof of custEM. Again, please write here if we should change it.
    \end{description}
    We could discuss if we should report CPU time instead (hence summing all parallel processes); but I think for the user of interest is the real-world time, how long do I have to wait, so I think it is good as we do it now. (Either way, the reader can make a quick estimation in one or the other direction; important is that we are consistent throughout the paper.)
    \imycom[RR]{Agreed.}
\end{itemize}
\itodo{\hfill$\Uparrow$\hfill$\Uparrow$\hfill List of things ToDo, Notes, etc\hfill$\Uparrow$\hfill$\Uparrow$\hfill~}
\newpage

\section{Introduction}

Controlled-source electromagnetic (CSEM) measurements are a frequently applied method in various geophysical exploration fields, such as geothermal, groundwater, oil and gas, mining, civil engineering, or geo-hazards. Modelling these electromagnetic fields is therefore of great interest to design survey layouts, to understand the measured data, and for inversion purposes. Publications regarding three-dimensional (3D) modelling in electromagnetic methods started to appear as early as the 1970's and 1980's. These early publications were integral equation (IE) methods, having an anomaly embedded within a layered medium, mostly for loop-loop type transient EM measurements \citep{GJI.74.Raiche, GEO.75.Hohmann, GJI.82.Das, GEO.86.Newman} and magnetotelluric (MT) measurements \citep{GEO.84.Wannamaker}. \cite{B.SEG.88.Ward} assemble many of these solutions in \emph{Electromagnetic Theory for Geophysical Applications}, which is widely viewed as an authoritative publication on electromagnetic geophysics.

In the 1990's, computer became sufficiently powerful that 3D modelling gained traction, and in 1995 the first International Symposium on Three-Dimensional Electromagnetics took place. This symposium resulted eventually in the book \emph{Three-Dimensional Electromagnetics} by the SEG \citep{B.SEG.99.Oristaglio}, and another book by \cite{B.02.Wannamaker} with the exactly same title came out only three years later. Often cited publications from that time are \cite{RSC.94.Mackie} for 3D MT computation; \cite{RS.94.Druskin} for frequency- and time-domain modelling using a Yee grid and a global Krylov subspace approximation; and \cite{RS.96.Alumbaugh, GJI.97.Newman} for low-to-high frequency computation on massively parallel computers.

The continuous improvement of computing power and the CSEM boom in the early 2000's in the hydrocarbon industry led to a wealth of developed numerical solutions and according publications. The most commonly applied methods to solve Maxwell's equation are the IE method \citep{GJI.74.Raiche, RS.02.Hursan, GEO.06.Zhdanov, GP.10.Tehrani, CAG.16.Kruglyakov, MGS.17.Kruglyakov} and different variations of the differential equation (DE) method, such as finite differences (FD) \citep{GEO.93.Wang, RSC.94.Mackie, RS.94.Druskin, GEO.09.Streich, CAG.13.Sommer}, finite elements (FE) \citep{GEO.04.Commer, GJI.11.Schwarzbach,GEO.12.daSilva, GJI.13.Grayver, GJI.13.Puzyrev, SEG.16.Zhang}, and finite volumes (FV) \citep{EM.90.Madsen, SIAM.01.Haber, PIER.01.Clemens, GEO.14.Jahandari}. There are also many different types of discretization, where the most common ones are regular grids (Cartesian, rectilinear), mostly using a Yee grid \citep{IEEE.66.Yee} or a Lebedev grid \citep{CMMP.64.Lebedev}, but also unstructured tetrahedral grids \citep{SEG.16.Zhang, CAG.17.Cai}, hexagonal meshes \citep{CAG.14.Cai}, or octree meshes \citep{ECP.07.Haber}.

Very well written overviews about the different approaches to 3D EM modelling are given by \cite{SG.05.Avdeev} and \cite{SG.10.Borner}. But there was also a tremendous publications output with regards to 3D EM modelling in the last 10-15 years, at least partly driven by the ever increasing computing power. One reason why there are so many publications about this topic results from the variety of techniques to solve the systems of linear equations (SLE). They can be distinguished between direct solvers \citep{GEO.09.Streich, GP.14.Chung, GEO.14.Jaysaval, GEO.15.Grayver, SEG.15.Oh, GJI.18.Wang}, indirect solvers \citep{GP.06.Mulder, GJI.15.Jaysaval}, or a combination of both, so-called hybrid solvers \citep{GEO.18.Liu}. The solvers often use preconditioners such as the multigrid method \citep{SIAM.02.Aruliah, GJI.16.Jaysaval}.

% The remaining paragraphs should be shortened/condensed

Many of the advancements made in the EM modelling community over the past decades have required that authors develop new implementations from scratch. These codes often provided the research group or company with a competitive advantage for a time, and thus the source codes were often kept internal. In some cases, executables have been made available for academic purposes upon request or to sponsoring consortium members. As the field continues to mature, advancements become more incremental, and particularly in an applied field, many advancements are driven by new use-cases and applications that were not considered by the original authors. In the aforementioned review on EM modelling and inversion Dmitry Avdeev concludes with the following statement: «\emph{The most important challenge that faces the EM community today is to convince software developers to put their 3-D EM forward and inverse solutions into the public domain, at least after some time. This would have a strong impact on the whole subject and the developers would benefit from feedback regarding the real needs of the end-users.}» Further, \cite{EXG.19.Oldenburg} argue that an open-source paradigm has the potential to accelerate multidisciplinary research by encouraging the development of modular, interoperable tools that are supported by a community of researchers.

Today it is becoming more common for researchers in many domains of science to release source-code with an open license that allows use and modification (e.g., see the Open Source Initiative approved licenses: \href{https://opensource.org/licenses}{opensource.org/licenses}). This is an important step for improving reproducibility of research, «\emph{to provide the means by which the reader can verify the validity of the results and make use of them in further research}» \citep{GEO.17.Broggini}. Going a step beyond releasing open-source software, many groups adopt an \emph{open model of development} where code is hosted and versioned in an online repository, all changes are public, and users can engage by submitting and track issues. Community oriented projects further engage by encouraging pull-requests, which are suggested changes to the code. Successful, well-maintained projects often have unit-testing and continuous integration that runs those tests with any changes to the code. Additionally, documentation that includes examples and tutorials is an important component for on-boarding new users and contributors. As a result, in many areas of the geosciences, we are seeing a shift away from a one-way distribution of (open-source) code towards building global communities around open projects. Other notable Python projects with an open model of development within the same realm as the codes under considerations are \texttt{pyGIMLi} \citep{CAG.17.Rucker}, \texttt{Fatiando} \citep{JOSS.18.Uieda}, \texttt{GemPy} \citep{GMD.19.DeLaVarga}, and \texttt{PyVista} \citep{JOSS.19.Sullivan}.

The open-source landscape in this field of research changed in the last five years quite dramatically. We introduce and compare four recent open-source projects with a focus on 3D CSEM modelling. All presented codes are in the Python ecosystem and use either the FV method on structured grids or the FE method on unstructured tetrahedral meshes. As such, we address the topic of validation. Analytical and semi-analytical solutions only exist for simple halfspace or layered-Earth models, which served mostly to verify new codes. Beyond these simple models, the only objective possibility to ensure the accuracy of solutions is by comparing results from different modellers. If different discretizations and implementations of Maxwell's equations yield the same result, it gives confidence in their accuracy. \cite{GJI.13.Miensopust} presents a review of two workshops dealing with the validation of magnetotelluric forward and inversion codes, but we are not aware of any comparable study or benchmark suite for CSEM data. This is, to the best of our knowledge, the first comparison exercise for CSEM codes. We hope our efforts help to ensure the reliability of simulations for any new or existing, open-source or closed-source 3D CSEM project.

We simulate EM fields for a layered background model with vertical transverse isotropy, with and without three resistive blocks, as well as for the complex marine, open-source MR3D model. These three models as well as the corresponding results from four different codes provide a benchmark for new (and existing) codes to be compared to and validated with. First we introduce the codes under consideration, and present afterwards the considered benchmark cases in detail together with the modelling results of our four codes in terms of accuracy and computational performance. Beyond that, we extensively discuss important points that control the performance and suitability of our FV and FE codes, including considerations about the mesh design and the choice of solvers. We conclude with a discussion and conclusions, and a motivation for the EM community at large to not only continue to extend the landscape of open-source codes but to also create a landscape of open-source benchmark models.

\section{Codes}

The four codes under consideration are, in alphabetical order, \custem \citep{GEO.19.Rochlitz}, \emg3d \citep{JOSS.19.Werthmuller}, \petgem \citep{CAG.18.CastilloReyes, GJI.19.CastilloReyes}, and \simpeg \citep{CAG.15.Cockett, CAG.17.Heagy}. All four codes have their user-facing routines written in Python, and all of them make heavy use of \texttt{NumPy} \citep{NAT.20.Harris} and \texttt{SciPy} \citep{NM.20.Virtanen}. The four of them are not just open-source but follow the open model of development, meaning that they come with both, an open-source license and an online-hosted version-control system with tracking possibilities (raising issues, filing pull requests). All developments comprise an extensive online documentation with many examples and have continuous integration to some degree. Newer package-management systems such as \texttt{conda}, \texttt{docker}, or \texttt{pip} significantly supported us to make our codes easily available and installable on any of the major operating system without any user-side compilations.

In the following, we provide a quick overview of the codes. It is, however, beyond the scope of this article to go into every detail of the different modellers, and we refer to their documentations for more details. An overview comparison of the different codes is given in Table~\ref{tbl:codecomparison}. All codes have in common that they solve Maxwell's equation in its differential form (DE) under the quasi-static or diffusive approximation, hence neglecting displacement currents, which is a common approximation for the low frequency range commonly applied in CSEM. The machines on which the different codes were run are listed in Table~\ref{tbl:machines} together with the responsible operator.
\imycom[DW]{I took out «the weak formulation of», as it actually doesn't apply to \emg3d. Please make sure that everything stated above («have in common that...») actually applies to your code.
\imycom[RR]{everything fits to custEM}
}
A few clarifying words on abbreviations and definitions: For the boundary conditions (BC) in the comparison table we use the abbreviations PEC and PMC, which stand for perfect electric conductor and perfect magnetic conductor, respectively. Another abbreviation used in the tables is dof for degree of freedom, which is equivalent to the size of the SLE we are solving. Finally, we use \emph{runtime} as the wall time or elapsed real time from start to end of solving the SLE; pre- and post-processing (e.g., mesh generation) is not measured. Note that the actual computation or CPU time is therefore much higher than the reported runtime for the codes that run in parallel. Memory refers to the maximum memory increase at any point of solving the SLE.

% Lindsey, I leave the SciPy solvers out. You mention that in practice they are not often used. emg3d can also use those solvers, and then multigrid is used as preconditioner. However, we cite SciPy, and as you say, SimPEG mostly uses PARDISO or MUMPS. To keep it simply I think we can leave SciPy out on the solver side. I mention it in the emg3d-section, if you want you can mention it in the SimPEG section too.
\tabl[bthp]{codecomparison}{Comparison of the four codes under consideration. Note that \emg3d is a solver on its own, while the other codes implement third-package solvers such as \texttt{PETSc} \citep{Preprint.Abhyankar}, \texttt{MUMPS} \citep{SIAM.01.Amestoy}, or \texttt{PARDISO} \citep{FGCS.04.Schenk}.}{
  \centering
  \footnotesize
\begin{tabularx}{\linewidth}{lXXXX}
  \toprule
  %
  & \custem & \emg3d & \petgem & \simpeg  \\
  \midrule
  %
  Home & \href{https://custem.rtfd.io}{custem.rtfd.io}
       & \href{https://empymod.github.io}{empymod.github.io}
       & \href{http://petgem.bsc.es}{petgem.bsc.es}
       & \href{https://docs.simpeg.xyz}{simpeg.xyz} \\
  License & GPL-3.0 & Apache-2.0 & GPL-3.0 & MIT \\
  Installation & \texttt{conda}
               & \texttt{pip}; \texttt{conda}
               & \texttt{pip}
               & \texttt{pip}; \texttt{conda} \\
  Comp. Dom. & frequency \& time & frequency & frequency & frequency \& time \\
  Method & FE & FV & FE & FV \\
  Mesh & tetrahedral & rectilinear & tetrahedral & recti- \& curvilinear \\
  BC & PEC; PMC & PEC & PEC & PEC; PMC \\
  Solver & \texttt{MUMPS} & \emg3d & \texttt{PETSc}; \texttt{MUMPS} &
           \texttt{PARDISO}; \texttt{MUMPS} \\
  %
  \bottomrule
\end{tabularx}}%
%

%
\tabl[bthp]{machines}{List of hardware, software, and operator with which the different codes were run.\itodo{Could everyone check if their RAM is GB or GiB?}}{
  \centering
  \footnotesize
  \begin{tabularx}{\linewidth}{lXl}
  \toprule
  %
  Code & Computer and Operating System & Operator \\
  \midrule
  %
  \custem & \textbf{PowerEdge R940 (server)}                  & Raphael Rochlitz \\
          & - 144 Xeon Gold 6154 CPU @2.666 GHz               & \\
          & - $\approx3$ TB DDR4 RAM                          & \\
          & - Ubuntu 18.04                                    & \\[.5em]
  \emg3d  & \textbf{Dell Latitude (laptop)}                   & Dieter Werthmüller \\
          & - i7-6600U CPU@2.6 GHz x4                         & \\
          & - 15.5 GiB RAM                                    & \\
          & - Ubuntu 18.04                                    & \\[.5em]
  \petgem & \textbf{Marenostrum4 (supercomputer)}             & Octavio Castillo-Reyes \\
          & - Intel Xeon Platinum from Skylake generation; 2 sockets Intel Xeon Platinum 8160 CPU with 24 cores each @2.10GHz for a total of 48 cores per node & \\
          & - 386 GB DDR4 RAM per node                        & \\
          & - SuSE Linux Enterprise                           & \\[.5em]
  \simpeg & \textbf{GKE n2-custom (Google cloud)}             & Lindsey Heagy \\
          & - Intel Cascade Lake, 8 vCPUs                     & \\
          & - 355 GB RAM                                      & \\
          & - Ubuntu 16.04                                    & \\
  %
  \bottomrule
\end{tabularx}}%
%


\subsection{custEM}

The customisable electromagnetic modelling Python toolbox \custem was developed for simulating arbitrary complex 3D CSEM geometries with a focus on semi-airborne setups, but it supports also land-based, airborne, coastal, and marine environments. Multiple electric or magnetic field or potential finite-element approaches were implemented as total or secondary field formulations. The finite-element kernel, including higher order basis functions and parallelisation, relies on the \texttt{FEniCS} project \citep{B.SPR.12.Logg, B.SPR.16.Langtangen}. The resulting SLEs are solved with \texttt{MUMPS}, which is a very robust but memory consuming choice. Primary field solutions are supplied by the \texttt{COMET} package \citep{GEO.20.Skibbe}.

The toolbox considers generally anisotropic petrophysical properties. Even though changes of the conductivity are mainly of interest for CSEM modelling, the electric permittivity and magnetic permeability can be taken into account using the preferred electric field approach on Nédélec elements. Recently, induced polarisation parameters in frequency-domain and three methods for simulating time-domain responses were added to \custem. The provided meshing tools are based on \texttt{TetGen} \citep{TOM.15.Si} and functionalities of \texttt{pyGIMLi} facilitate the generation of tetrahedral meshes including layered-earth geometries with topography or bathymetry and anomalous bodies which are allowed to be connected or to reach the surface.

\subsection{emg3d}

The 3D CSEM code \emg3d is a multigrid solver \citep{CMMP.64.Fedorenko} for electromagnetic diffusion following \cite{GP.06.Mulder}, with tri-axial electrical anisotropy, isotropic electric permittivity, and isotropic magnetic permeability. The matrix-free solver can be used as main solver, or alternatively as preconditioner for one of the Krylov subspace methods implemented in SciPy. The governing equations are discretized on a staggered grid by the finite-integration technique \citep{AEU.77.Weiland}, which is a finite-volume generalisation of a Yee grid. The code is written completely in Python using the NumPy and SciPy stacks, where the most time- and memory-consuming parts are sped up through jitted (just-in-time compiled) functions using Numba \citep{LLVM.15.Lam}. The strength of \emg3d is the matrix-free multigrid implementation, which is characterised by almost linear scaling both in terms of runtime and memory usage, and it is therefore a solver that uses comparably very little memory.

As such the code is, contrary to the other three, primarily a solver, which solves the Maxwell's equation for a single frequency and a single, electric source. However, recent developments added functionalities such that \emg3d can be used directly as a more general EM modeller too. It features now routines to obtain the magnetic field due to an electric source, to obtain the electric and magnetic fields due to a magnetic source, and also to obtain time-domain responses. For the underlying discretization the Python package \discretize is used, which is part of the larger \simpeg ecosystem. Interoperability between \simpeg and \emg3d is therefore straightforward.

\subsection{PETGEM}

\petgem is a parallel code for frequency-domain 3D CSEM data for marine and land surveys. The high-order edge FE method (HEFEM) is used for the discretization of the governing equations in its diffusive form. This technique provides a suitable mechanism to obtain stable numerical solutions as well as a good trade-off between number of dof and computational effort. The current implementation supports up to sixth-order tetrahedral vector basis functions. Moreover, because the HEFEM belongs to the FE family, the unstructured meshes can be used efficiently on complex geometries, e.g., models with topography and bathymetry.

\petgem permits to locate the source and receivers anywhere in the computational domain (e.g., sediments, seafloor, sea, ground), allowing to analyse the physical environment of the electric responses and how parameters impact them (e.g., frequency, conductivity, dependence on mesh setup, basis order, solver type, among others). Furthermore, \petgem implements a semi-adaptive mesh strategy ($hp$ mesh refinement) based on physical parameters and on polynomial order to satisfy quality criteria chosen by the user. Nonetheless, only Horizontal Electric Dipole (HED) has been implemented.

For the parallel forward modelling computations, a highly scalable MPI (message passing interface) domain decomposition allows reducing runtimes and the solution of large-scale modelling cases. This strategy is capable of exploiting the parallelism offered by both modest multi-core computers and cutting-edge clusters (e.g., High-Performance Computing architectures).

% Octavio: We removed the outlook part, we did the same for custEM and emg3d. I hope that is OK.
% Based on the current state of the \petgem project, there are many possibilities to improve or to add additional features. For instance, in the short-term, more modelling routines and support for multi-source set-ups will be implemented. For the long-term, the obvious next step is to move to 3D CSEM data inversion.

\subsection{SimPEG}
\todo{Please condense SimPEG a bit; the other codes use about 200-240 words, SimPEG currently close to 400.}
\simpeg is a modular toolbox for simulations and gradient based inversions in geophysics. Current functionality includes modelling and inversion capabilities for gravity, magnetics, direct current resistivity, induced polarisation, electromagnetics (time and frequency, controlled and natural sources such as magnetotellurics), and fluid flow (Richards equation). It is a community driven project that aims to support researchers and \mycom[DW]{I added «and practitioners», as I think «support researchers» downplays the fact that \simpeg is used by various companies as well\dots}practitioners by providing a flexible, extensible framework and to facilitate integration of data sets from a variety of geophysical methods, including joint inversions \citep{GJI.20.Astic}, by providing a common interface to each method. 

Meshes and finite volume differential operators are implemented in the \discretize package, which currently includes tensor, octree, cylindrical, and logically rectangular meshes. Each mesh type inherits from a common structure and uses the same naming conventions for methods and properties. This allows us to decouple the implementation of a discretized partial differential equation from the details of the mesh geometry and therefore we can write a single implementation of discretized partial differential equation (or set of partial differential equations) in \simpeg that will support all mesh types implemented in \discretize.

The electromagnetic implementations use a staggered grid finite volume approach where physical properties are discretized at cell centres, fields on cell edges and fluxes on cell faces.
%(for an overview of discretizing Maxwell's equations, we recommend \cite{B.SIAM.14.Haber}).
There are two different discretization strategies implemented for Maxwell's equations: (I) the EB-formulation, which discretizes the electric field ($\vec{e}$) on cell edges and the magnetic flux density ($\vec{b}$) on cell faces, and (II) the HJ-formulation, which discretizes the magnetic field ($\vec{h}$) on cell edges and the current density ($\vec{j}$) on cell faces. The physical properties electrical conductivity / resistivity and magnetic permeability are discretized in both cases at cell centres. Having multiple implementations allows for testing that compares results from each approach, as well as the representation of both electrical and magnetic sources on cylindrically symmetric meshes. \simpeg supports variable magnetic permeability and full-tensor anisotropy for the physical properties.

\simpeg interfaces to various solvers including \texttt{PARDISO} and \texttt{MUMPS}, and also to some implemented in SciPy. For problems with many sources (e.g. airborne EM), \simpeg contains machinery for domain-decomposition approaches; see for example \cite{GEO.20.Fournier}.

\section{Numerical Validation}

We computed the responses for three different models to validate that the four 3D CSEM codes yield the same electromagnetic responses and to compare different solver types (FE, FD) and different mesh types (unstructured tetrahedra, tensor, octree) in different scenarios. The first model is an anisotropic layered model, which can be compared to semi-analytical solutions. The layered model serves also as background model for the second validation, where we add three resistive blocks into the subsurface. The final comparison is based on the realistic, anisotropic marine model MR3D, which is an open resistivity model which comes with computed CSEM responses.

\subsection{Layered Model}


The layered (1D) model consists of an upper halfspace of air ($\rho_\text{air}=\num{e8}\,\ohmm$), a 600\,m deep water layer ($\rho_\text{sea}=\num{0.3}\,\ohmm$), followed by a 250\,m thick, isotropic layer of 1\,\ohmm, a 2.3\,km thick, anisotropic (vertical transverse isotropy, VTI) layer of $\rho_\text{h}=2\,\ohmm$ and $\rho_\text{v}=4\,\ohmm$, and finally a resistive, isotropic basement consisting of a lower halfspace of $1000\,\ohmm$. The survey consists of a 200\,m long and 800\,A strong source at a single position with frequency $f=1\,$Hz, and three receiver lines of 101 receivers each. The centre of the $x$-directed source is at $x=y=0\,$m, 50\,m above the seafloor. The $x$-directed receivers are placed on the seafloor every 200\,m from $x=-10\,$km to $x=+10\,$km in three lines with $y=-3; 0; +3\,$km.


A layered model fails to show the strength of 3D modellers, and in reality one would not choose a 3D code to compute responses for a layered model. However, it is one of the few examples that can be compared to semi-analytical results, and is therefore still a valuable test. All layered-model responses are compared to results computed with the semi-analytical code \empymod\citep{GEO.17.Werthmuller}.

Figure~\ref{fig:results-layered} shows the result of the layered model verification, where the left, middle, and right columns are for the real part, imaginary part, and absolute value, respectively (dashed lines indicate negative values). The top row shows the actual, semi-analytically computed responses, and the second and the third rows show the relative error (in percent) of the inline and the broadside receivers, respectively (note that for the 1D case the two broadside lines for $y=\pm3\,$km are identical).

%
\plot*{results-layered}{width=\textwidth}{Inline and broadside responses (top row) for the layered model. The relative error of the four 3D modellers are shown in the second and the third row for the inline and broadside responses, respectively.}
%

There are a few noteworthy points:
\begin{enumerate}
  \item The error plots of the real and imaginary responses are  dominated by the zero-crossings, which blows the error up as is the nature of the relative error formula, it has no real-world implications.
  \item However, it can be seen from the absolute-values errors that the error is generally in the order of 1\,\% or less.
  \item The relative errors close to the source are getting huge, particularly for the inline receivers and particularly for the results of \emg3d and \simpeg. These comparably huge errors are caused by a too coarse discretization around the source. They could be significantly reduced by choosing a finer discretization, at the cost of increased computation time.
  \item The relative error of the responses from \custem and \petgem, on the hand, are relatively big at large offsets, particularly for the inline real responses. This has, again, its origin in the chosen discretization. The thin isotropic layer beneath the water requires many tetrahedra for a good-quality discretization. Therefore, the FE codes only extend this thin layer up to 30\,km for the origin, surrounded by a ten times larger halfspace-like boundary mesh with water conductivities assigned to the subsurface. This approximation significantly reduced the problem size compared to considering the layered-earth geometry for the whole computational domain.
\end{enumerate}

Please not that \emg3d and \simpeg used the same tensor mesh in this example.
\imycom[DW]{Lindsey: A sentence why Tensor was chosen over OcTree here?}

The runtime and memory requirements for the layered model are listed in Table~\ref{tbl:comp-layered}.

%
\tabl[bthp]{comp-layered}{Comparison of number of processes, runtime, and memory, as well as the degree of freedom of the discretization used by the different codes for the layered model.}{
  \centering
\begin{tabular}{lrS[table-format=6.0]S[table-format=4.1]S[table-format=8.0]}
  \toprule
  %
       & \#Procs & {Runtime (s)} & {Memory (GiB)}   & {\#dof} \\
  \midrule
  %
  \custem & 24 &   118 &  97.8 & 1530808 \\ % dof: 3061616 / 2 (Real+Imag -> Complex)
  \emg3d  &  1 &   116 &   0.4 & 4813216 \\
  \petgem & 24 &    90 &  34.2 & 2455868 \\
  \simpeg &  4 & 10089 & 280.3 & 4813216 \\ % nprocs: 1*4 threads
  %
  \bottomrule
\end{tabular}}%
%

\subsection{Block Model}

The block model is a derivation of the \emph{Dublin Test Model 1} from the first EM modelling workshop described by \cite{GJI.13.Miensopust}. We use the same layout of the blocks but adjust the dimensions and resistivities to a typical marine CSEM problem, as shown in Figure~\ref{fig:model-block}. Additionally, we add our \emph{Layered Model} as a VTI background. The three resistive blocks have resistivities of $\rho=10\,\ohmm$ (shallow beam perpendicular to survey lines), $\rho=100\,\ohmm$ (thin plate, South-East), and $\rho=500\,\ohmm$ (cube, North-West).
%
\plot*{model-block}{width=.6\textwidth}{Sketch of the block model, consisting of the layered background model with three resistive blocks, embedded in the thick background layer which has VTI with $\lambda=\sqrt{\rho_\textrm{v}/\rho_\textrm{h}}=\sqrt{2}$.}
%

There are no (semi-)analytical solutions for such a model. We therefore use the normalised root-mean square difference (NRMSD) between the results of different codes, where the NRMSD of two responses $R_1$ and $R_2$ is given by
%
\begin{equation}
  \mr{NRMSD~(\%)} = 200 \frac{|R_1 - R_2|}{|R_1| + |R_2|}\ .
  \label{eq:nrmsd}
\end{equation}
%

The results for the three receiver lines $y=-3,0,+3\,$km are shown in the left, middle, and right columns of Figure~\ref{fig:results-block}, respectively. The top row shows the result of \emg3d, as an example, and the bottom row shows the NRMSDs between the absolute responses of the codes.
%
\plot*{results-block}{width=\textwidth}{Results of the block model comparison: The responses of \emg3d, as an example, are shown in the top row, and the NRMSDs (\%) between the absolute responses ($|E_x|$) of the different codes are shown in the bottom row.}
%

There are a few noteworthy points:

\begin{enumerate}
  \item The error levels are overall comparable; the NRMSD is generally below a few percent, increasing towards larger offsets.
  \item The NRMSD between the FV codes \emg3d and \simpeg is comparably low except at the boundary. The generally low NRMSD is because these codes use the same rectilinear mesh for modelling. The increasing NRMSD towards the boundary is because \emg3d uses the PEC boundary condition, while \simpeg uses the PMC boundary condition.
  \item The FE codes \custem and \petgem show similar boundary effects, but are influenced by the anomaly response (higher errors on right side and now unsymmetrical error distribution).
  \item The agreement within the same differential equation method (FE or FV) is better than across methods. 
\end{enumerate}

The corresponding required runtime and memory are listed in Table~\ref{tbl:comp-block}.
%
\tabl[bthp]{comp-block}{Comparison of number of processes, runtime, and memory, as well as the degree of freedom of the discretization used by the different codes for the block model.}{
  \centering
\begin{tabular}{lrS[table-format=6.0]S[table-format=4.1]S[table-format=8.0]}
\toprule
  %
       & \#Procs & {Runtime (s)} & {Memory (GiB)}   & {\#dof} \\
  \midrule
  %
  \custem & 24 &   158 & 115.9 & 1654242 \\ % dof: 3308484 / 2  (Real+Imag -> Complex)
  \emg3d  &  1 &   115 &   0.4 & 4813216 \\
  \petgem & 24 &    72 &  35.8 & 2455868 \\
  \simpeg &  4 & 10295 & 280.4 & 4813216 \\ % nprocs: 1*4 threads
  %
  \bottomrule
\end{tabular}}%
%

\imycom[RR]{@ Octavio: I think you use your own mesh here? Do you use the iterative solver in this example? otherwise, the values in the table do not make too much sense for me...}

\subsection{Marlim R3D}

The Marlim oil field is a giant reservoir in a turbidite sandstone horizon in the north-eastern part of the Campos Basin, offshore Brazil, which was discovered in 1985. \cite{BJG.17.Carvalho} created from seismic data and well log data a realistic, three-dimensional resistivity model with vertical transverse isotropy (VTI), called MR3D, which they released under the open creative common license \emph{CC BY 4.0}. \cite{GEO.19.Correa} computed CSEM data for MR3D for six frequencies from 0.125\,Hz to 1.25\,Hz, and released them under the same CC license. To compute the data they used a state-of-the-art code from the industry \citep[][ \emph{SBLwiz} software from \emph{EMGS}]{GEO.07.Maao}. It is therefore on one hand an ideal case to validate our open-source codes against, as it is a complex, realistic model and the data were computed by an industry-proofed code. On the other hand it is impossible to reproduce it exactly, as it is a closed-source code and we cannot know exactly what was done internally. Additionally, that code is a time-domain code, including the air layer via a non-local boundary condition at the water-air interface \citep{GEO.10.Mittet}, whereas the four codes under consideration here compute the results all in the frequency-domain.

The full MR3D model consists of $1022 \times 371 \times 1229$ cells, totalling to almost 466 million cells, where each cell has dimensions of 25 by 75 by 5\,m. For the computation the model is upscaled to 515 by 563 by 310 cells, totalling to almost 90 million cells, where each cell has dimensions of 100 by 100 by 20\,m. Both the full model and the upscaled computational model are released openly. The published data set consists of a regular grid of receivers of 20 in eastern direction by 25 in northern direction, 500 receivers in total, with 1\,km spacing located on the irregular seafloor. 45 source-towlines were located on the same grid above each receiver line, 50\,m above the seafloor, with shots every 100\,m. The computed responses for one of the receivers, \texttt{04Rx251a}, are shown in the original paper for the East-West inline-source \texttt{04Tx013a} and the East-West broadside-source \texttt{04Tx014a} (broadside offset of 1\,km). We reproduce the responses for this receiver and corresponding source-lines in our comparison. The $x$-$z$ cross-section of the horizontal resistivity model at the receiver position is shown in Figure~\ref{fig:model-marlim}, together with the receiver and sources positions. All layer have a VTI with $\lambda=\sqrt{2}$, except for the air layer, the seawater, and the salt layer, which are all isotropic.

%
\plot*{model-marlim}{width=\textwidth}{MR3D horizontal resistivity model, $x$-$z$-slice through the receiver $y$-position and with major formations annotated. Air (not shown in the model), seawater, and the salt layer are electrically isotropic, everything else has VTI with $\lambda=\sqrt{2}$. The receiver is located on the seafloor, and the sources fly 50\,m above the seafloor.}
%

The responses for all six frequencies and all three electric components are shown in Figure~\ref{fig:results-marlim-responses}, both inline and broadside. The published responses are shown in color (and with markers), and the responses from our codes are shown beneath in grey colours in this overview plot.
%
\plot*{results-marlim-responses}{width=\textwidth}{MR3D comparison between our codes (grey lines) and the published data (coloured lines). The grey lines under the coloured lines are not visible in most areas, meaning they are very similar. However, there are three notable zones, (1) to (3), which are explained further in the text.}
%
Two datasets were published, one containing clean responses, and one where realistic noise was added. For the comparison we use the clean data, but we indicate the chosen noise level by the horizontal line at \num{2e-15}\,V/m. It can be seen that the grey lines are not visible for most parts, which means that the data agree well. However, there are a few notable points where this is not the case, and they are marked with numbers in the figure.
\begin{enumerate}
  \item There are some noticeable differences in the positive offsets of the broadside $E_y$ and $E_z$ components. The differences are related to the bathymetry. And the reason why we cannot reproduce the published results exactly is that the code is not open-source, and we do not exactly know what it does internally. See the discussion around Figure~\ref{fig:results-marlim_survey}.
  \item The highest three frequencies became noisy at large offsets for all of our codes (well below any real-world noise level). The reason why the published responses are not noisy is probably because they were computed in the time domain, and transformed to the frequency domain. This explains their smooth behaviour, without saying anything about their accuracy.
  \item The responses for the $E_y$ component of the inline acquisition line do not agree at all. In the 1D case, the inline $E_y$ component would be zero, and the only response we can measure here come from 3D effects, but they are very low, roughly two orders of magnitude lower than the $E_x$ component. The published responses are not stable either, and therefore any of the responses is as bad as the other. Also, tiniest differences in meshing will have a huge effect here.
\end{enumerate}

Figure~\ref{fig:results-marlim_2published} shows the NRMSDs between the published results and our codes for the three strongest components, the inline $E_x$ and the broadside $E_x$ and $E_y$ components, for three frequencies.
%
\plot*{results-marlim_2published}{width=\textwidth}{NRMSDs for all four codes in comparison with the published data, for the inline $E_x$ field and the broadside $E_x$ and $E_y$ fields in the left, middle, and right column, respectively. Shown are the three frequencies 0.125\,Hz, 0.5\,Hz, and 1.0\,Hz in the top, middle, and bottom row, respectively.}
%
A few general conclusions can be drawn from this figure: In general the NRMSD is roughly 10\,\% or less, particularly for the inline $E_x$ component it is only a few \% or less. In some cases the regular meshes with \emg3d and \simpeg have a smaller NRMSD, such as, e.g., negative offsets in the inline and broadside $E_x$ responses, in some cases the unstructured tetrahedral meshes with \custem and \petgem have a smaller NRMSD, such as, e.g., positive offsets for all broadside $E_y$ responses, and often they have a comparable NRMSD. Some NRMSD have an interesting step-pattern, which is more pronounced for \emg3d and \simpeg than for \custem and \petgem; this is related to the annotated point (3) in Figure~\ref{fig:model-marlim}, and will be discussed in more detail with Figure~\ref{fig:results-marlim_survey}.

The importance of the meshing to the result can be seen in Figure~\ref{fig:results-marlim_2ours}, where we compute the NRMSDs between our codes.
%
\plot*{results-marlim_2ours}{width=\textwidth}{NRMSDs, just as in Figure~\ref{fig:results-marlim_2published}, but comparing some of the four codes with each other. Good visible is that \custem and \petgem produce very similar results, which is due to the fact that they use the same mesh.}
%
The most obvious result in that figure is that \custem and \petgem produce results that are almost identical, their NRMSD is generally below 0.03\,\%. The reason is simple: \custem and \petgem use the exactly same mesh for the computation, the only difference is therefore the solver. As they are so similar we compare \emg3d and \simpeg only to \custem in Figure~\ref{fig:results-marlim_2ours}, as the comparisons to \petgem would look the same. But in general the NRMSDs between our codes look similar as in the comparison with the published results, which is due to the different meshes in use.

The actual differences in meshing is shown in the next two figures, in Figure~\ref{fig:results-marlim_entire} for the entire computational domain, and in Figure~\ref{fig:results-marlim_survey} a zoom-in to the survey domain, the domain of interest.
%
\plot*{results-marlim_entire}{width=\textwidth}{Meshes (left column), $E_x$ field (middle column), and $E_y$ field (right column) for a tensor mesh (top row), octree mesh (middle row), and tetrahedra mesh (bottom row) for the entire computational domain.}
%
The top rows show the tensor mesh of \emg3d, the middle rows the octree mesh of \simpeg, and the bottom rows the tetrahedra mesh of \custem and \petgem. The actual mesh, the $|E_x|$ field, and the $|E_y|$ field are shown in the left, middle, and right columns, respectively.

\itodo{We have to decide which figures to include, please state your preference:
\imycom[DW]{I would go for at least two figures, probably 9 and 10 (in reversed order); in my opinion these are the most interesting figures, and I have never seen a comparison like this.}
\imycom[RR]{only keep this figure in my opinion (Figure 9), remove 8, 10 \& 11. One figure should be enough for an illustrative field and mesh representation.}
}

%
\plot*{results-marlim_survey}{width=\textwidth}{Meshes (left column), $E_x$ field (middle column), and $E_y$ field (right column) for a tensor mesh (top row), octree mesh (middle row), and tetrahedra mesh (bottom row) for the survey domain.}

The most striking point of these mesh comparison plots is that, while the overall fields are similar and particularly the responses at receiver locations are the same, the fields can vary quite a lot in different parts of the model. At, e.g., two kilometers below the seafloor there are already significant changes visible, purely due to meshing. This is an important insight that has to be taken into account when interpreting inversion results.

The required runtime and memory to compute the shown MR3D responses for the four codes are listed in Table~\ref{tbl:comp-marlim}. The memory requirement varies from $0.5-520\,$GiB, and the runtime from roughly 7\,min to 1h\,23\,min, where the codes were run on very different machines from servers to supercomuters (see Table \ref{tbl:machines}) and use between 1 and 96 processes in parallel).

\plot*{results-marlim_medium}{width=\textwidth}{Additional zoom “medium”; which ones should we use?}
\plot*{results-marlim_small}{width=\textwidth}{Additional zoom “small”; which ones should we use?}
%
\tabl[bthp]{comp-marlim}{Comparison of number of processes, runtime, and memory, as well as the degree of freedom of the discretization used by the different codes for the MR3D model.}{
  \centering
\begin{tabular}{lrS[table-format=6.0]S[table-format=4.1]S[table-format=8.0]}
  \toprule
  %
       & \#Procs & {Runtime (s)} & {Memory (GiB)}   & {\#dof} \\
  \midrule
  %
  \custem & 64 & 4034 & 522.7 & 4539474 \\ % dof: 9078948 / 2 (Real+Imag -> Complex)
  \emg3d  &  1 & 1200 &   0.5 & 5998992 \\
  \petgem & 96 & 3527 & 437.8 & 4539474 \\
  \simpeg &  4 &  422 &  12.8 &  720146 \\ % nprocs: 1*4 threads
  %
  \bottomrule
\end{tabular}}%
%
\todo{Update nc-file of custEM for new CPU/RAM.}


\section{Discussion}
\imycom[DW]{Currently still a collection of discussions. Needs better structuring.}

Even if modelling results may look completely valid, they could not be correct at all. Validating the performance of 3D CSEM codes for real 3D problems is only possible by cross-validating multiple solutions. Overall, we observed an excellent match between all solutions. The relative misfits, almost always smaller than 10\,\%, were related to the either weak or strong amplitudes of real or imaginary parts and can be attributed to particularities regarding the discretization, boundary effects, and interpolation. Considering this satisfactory result in terms of accuracy and robustness, it is more interesting to discuss the characteristics of the four considered codes with focus on the demand of computational resources and the capability of handling the investigated CSEM geometries, including the very important underlying mesh design. The following arguments are based on a comparison of only marine CSEM models, which are the focus of industry applications. Nevertheless, we are confident that many observations would hold in a similar manner for land-based, airborne, or mixed setups.

For the MR3D results one might argue that 5-10\,\% NRMSD is a lot. We argue that this is probably as good as it gets, given that the code with which we compare is not open-source. This has been shown within the process of these results. Our first attempt were based on the originally published, fine MR3D model. This model is very detailed, but too small in x- and y-direction for CSEM computation, and each of our codes did its own extrapolation. However, none of the codes were able to reproduce the published results. The reason is that there are many ways how you can extrapolate a model. We reached out to the authors of MR3D, and they kindly agreed to also published the upscaled and extended computational model. It is only with this model that we were able to get comparable results, visually mostly the same as shown in Figure~\ref{fig:model-marlim}.

We point out that making a synthetic and a real benchmark model available to cross-validate any 3D CSEM code was the determining reason for their choice, regardless of the suitability of the considered FD and FE codes for these problems. The first model, including the block-anomalies variation, is exactly reproducible by using any numerical method with either regular or irregular computational domains. Aside from the idea of analysing an industrially relevant benchmark example, the MR3D model was also considered since the primary design from \cite{GEO.19.Correa} uses a regular discretization. At least theoretically, the exact geometry might be reconstructed with unstructured meshes, but it is never possible to convert an irregular geometry to a regular grid without approximations. Hence, the chosen examples favoured the FD codes, since they used either regular grids or octree-meshes, whereas the two FE codes were required to tetrahedralise the model geometries.

In the first model, the necessity of discretizing regular structures with tetrahedra, especially thin layers, led to comparatively large FE problem sizes and computational resource demands. FE codes with unstructured meshes were probably the poorest choice of all commonly used methods for this setup in terms of computational performance. However, thinking about introducing just one slight irregularity such as bathymetry, a thin dipping plate or an anticline, the performance behaviour could likely switch. All of these changes would barely change the problem size for the FE codes, but significantly increase it for the FD codes if the true geometry should be approximated sufficiently accurate.

The real MR3D model was, ironically, the ideal case for FE codes because it's structure was characterised by irregular lithological horizons obtained from seismic data. Since the corresponding published resistivity model was defined on a regular grid, we were forced to approximate the comparatively fine regular discretization by an unstructured one and interpolate the resistivity data for being able to apply the FE codes to this problem. We are completely aware about the ineptness of this re-approximation procedure, unless it served for the cross-validation purposes. Nevertheless, the chosen tetrahedral discretization, especially in combination with second order basis functions, showed clearly the intrinsic strength of the FE codes, requiring significantly less dof for meshing irregular geometries in general. The p2 FE solution required only $\approx$9\,M dof instead of $\approx$90\,M dof for the FD system by \citet{GEO.19.Correa}, disregarding the structure of the system matrices. It is important to state that the FE mesh used for the MR3D model incorporates the entire domain and could be used for all source and receiver positions, whereas the rectilinear and octree meshes were designed for the shown receiver and two source lines.

The codes under consideration use direct and indirect solvers. In general, the biggest advantage of iterative solvers with appropriate preconditioners are the comparatively small memory requirements. Direct solvers can be considered as most robust to solve any ill-conditioned SLE, which is most important for systems of the FE method on unstructured meshes. As the consumed computational resources indicate, a more powerful machine than a laptop is required for most 3D CSEM problems, but no high-performance-computing architecture.

The computation times can differ significantly for specific simulations, but there is no clear general advantage for one of the solver types. In our models, the runtime comparison favoured the iterative solver and therefore, \emg3d. Even though not shown here, direct solvers exhibit their strongest advantage in terms of computation times if responses of multiple CSEM transmitters need to be computed in the same computational domain. As the system matrix factorisation requires 98-99\,\% of the solution time, computations for additional sources come at almost no cost for direct solvers, i.e., MUMPS, whereas it would come at the same cost as the first source for \emg3d. For the sake of completeness, note that there are also advanced techniques for iterative solvers to make use of preconditioners, factorisations, or intermediate solutions of the previous SLE to speed up the computation for multiple sources, but this argumentation is beyond the scope of this work.
\imycom[RR]{maybe add also another important argument: Direct solvers ALWAYS solve solvable systems, whereas iterative solvers with suited preconditioners heavily rely on the matrix structure and condition caused influenced by resistivity distributions and cell/element sizes/shapes. This is in my opinion also the most important point why most of the recent unstructured FE papers use direct solvers (new real-world problem and your preconditioner might be completely useless even if it worked for your paper example - @ Octavio - can you confirm this thought or am I telling nonsense?
\imycom[DW]{If this is true we should definitely include it. However, this is new to me (and I currently doubt it), we would need a reference confirming this.}
}

We see this work as the start for future elaboration on the validation of modelling codes, not necessarily restricted to CSEM problems. Much more comparisons and examples are required which not only result in the finding that a sufficient accuracy was observed. 3D CSEM modelling is a difficult task, which requires many considerations. It starts with the selection of the right code for the problem. We found that the meshing task, which is most performance determining, is particularly difficult and relevant, choosing cells small enough to appropriately represent the model yet to be as coarse as possible still achieving the desired precision. The required model extent has to be considered as well, thinking also about effects of the so-called airwave in marine setups or boundary meshes in general. This is not a new finding but rather well known fact. We are surprised that only the minority of publications in the field of 3D EM modelling considers a sufficiently detailed elaboration of the meshing process.

A completely objective evaluation of codes is only possible by cross-validation. Nevertheless, we point out that using different discretizations or formulations of the numerical approach, e.g. by choosing field or potential approaches, second or total field formulations, different polynomial order basis functions, and others, is a suitable alternative to the cross-comparison of multiple codes for self-validating complex 3D results. This is a very powerful method to confirm the general functionality of codes for any modelling problem, which might not be covered by existing cross-validation benchmark examples. Considering for instance  this study, it would be very optimistic to estimate the accuracy of the utilised codes for a land-based mineral exploration setup with a highly-conductive, steeply-dipping conductor. However, each modeller could run multiple simulations with different configurations to increase the reliability of the obtained results.

It is worth noting that the reported runtime is just one of the aspects, and we would like to emphasise that neither the runtime nor the memory consumption are at the core of this comparison, but the validation of the results. As such no special efforts were undertaken to minimise either, as this is an entire different task. Another aspect is to generate an appropriate mesh, with all the testing involved, can easily take days of work. As such the computation times becomes relative, and the ease of usability of a code has to be considered too. For \custem, \petgem, and \simpeg it is therefore also a question of how easy everything is to set-up, because after the set-up phase it depends largely on the used solver, which might change in the future; \simpeg, e.g., is looking at moving to similar solvers as currently are used by \custem and \petgem. It is, however, slightly different for \emg3d, as that is primarily a solver on its own. It is also important to state that while \emg3d, \custem, and \petgem are purely CSEM codes, \simpeg is a much bigger framework which includes other geophysical methods as well.

The shown examples consider the geophysical problem of marine CSEM with resistive bodies. We hope to see in the future similar comparisons for other cases, such as land CSEM with strong topography looking for conductive bodies. The used codes could, and many frequently are, applied to very different applications, including any other geophysical exploration in the sea or on land (oil, mining, geothermal, groundwater, environmental) and also non-geophysical tasks. The current main applications of the different codes are mining and environmental (\custem, \simpeg), oil and more recent medical and geothermal (\petgem), and oil, methane, and deep sea mining (\emg3d).

A plus-point of a collaboration between different projects such as this is that it brings the realm as a whole further, which should be motivation enough for further collaborations. Within \simpeg, this work has motivated feature development including interpolation and averaging strategies from mapping physical properties on a fine mesh to a coarser mesh for computation and new examples to include in the documentation for designing octree meshes. Furthermore, as a part of a broader development objective of inter-operating with other forward-simulation engines, connecting \emg3d and \simpeg provided a motivating use-case for the latest refactor and release of \simpeg 0.14.0. Within \emg3d, this work pushed a lot of the meshing functionality, and implementing of I/O utilities for different file formats. Within \custem, cell-wise resistivity interpolation was added for this work, multi-layer subsurface topography was the first time applied as well as first time reciprocity modelling, and also improved mesh design (resolution, refinement, etc.).

We hope that these results may be useful for the entire CSEM community at large, and we invite and encourage the community to make more code, modelling scripts, and results publicly available.

\section{Conclusions \& Outlook}
\imycom[DW]{Not really conclusions, but a mix with discussions. Make pure conclusions.}

We compared four different open-source 3D CSEM modellers by computing responses for a layered, anisotropic model, a model containing three resistive blocks, and the realistic marine Marlim R3D model. All comparisons exhibited an excellent reliability of the solutions, in the order of $\approx1\,\%$ misfit for the layered and the block model, and $\approx10\,\%$ for the real world model. Our data should make it very easy for new codes to have a readily available data set to test against and validate. We are confident that the discussions about runtimes, memory consumption, solver choices, and, most important, discretizations of the 3D CSEM problems help potential users not only to decide which modeller is best suited for specific tasks, but also to support their individual projects.

As none of the considered methods is best suited for all problems, it is important to have more test models for various scenarios in future. Our study is limited to marine CSEM cases in the frequency-domain. Reasonable extensions include providing benchmark models for land, airborne, or mixed CSEM environments as well as time-domain data. Another addition to this work would be focusing on complex irregular  models, tailored for an FE or FV code based on unstructured meshes, and to compare how the FD codes can cope with it.

We encourage the community to not only work on new open-source code developments but also to create a landscape of easily accessible benchmark models for increasing the number of reliable and reproducible solutions. Our study could provide one of the primary inputs for this task. We are confident that the mentioned development will and should be taking place, taking into account the current general trend in science to open-source papers, codes or data.




\section{Acknowledgement}

We would like to thank Paulo Menezes for the help and explanations with regards to the Marlim R3D model and corresponding CSEM data, and for making their actual computation model available under an open-source license.

The work of D.W. was conducted within the Gitaro.JIM project funded through MarTERA, a \emph{European Union's Horizon 2020} research and innovation programme, grant agreement N$^\circ$~728053; \href{https://www.martera.eu}{martera.eu}.

The development of \custem by R.R. as part of the DESMEX/DESMEX II projects was funded by the Germany Ministry for Education and Research (BMBF) in the framework of the research and development program Fona-r4 under grants 033R130D/033R130DN.

The work of O.C-R. has received funding from the \emph{European Union's Horizon 2020 programme} under the \emph{Marie Sklodowska-Curie} grant agreement N$^\circ$ 777778. Further, the development of \petgem has received funding from the \emph{European Union's Horizon 2020 programme}, grant agreement N$^\circ$~828947, and from the Mexican Department of Energy, CONACYT-SENER Hidrocarburos grant agreement N$^\circ$ B-S-69926. Furthermore, O.C-R. has been 65\% cofinanced by the European Regional Development Fund (ERDF) through the Interreg V-A Spain-France-Andorra program (POCTEFA2014-2020). POCTEFA aims to reinforce the economic and social integration of the French-Spanish-Andorran border. Its support is focused on developing economic, social and environmental cross-border activities through joint strategies favouring sustainable territorial development.


\section{Data}

All files to rerun the different models with the four codes and reproduce the shown results are available at \dots.
\itodo{Put up on Zenodo, link here.}


% REFERENCES
\bibliographystyle{Refs}  % Modified SEG bibliography style.
\bibliography{Refs}

\end{document}
